{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from cnnArchitectures.Xception import get_xception_model\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-279eb7f22ada>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mimage_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\".bmp\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdata_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "# E.g. D:\\DATASETS\\ALL_2\\training\\fold_0\\all\n",
    "dataset_path = 'D://DATASETS/ALL_2'\n",
    "data_paths = [\n",
    "    dataset_path + '/training/fold_0/',\n",
    "    dataset_path + '/training/fold_1/',\n",
    "    dataset_path + '/training/fold_2/'\n",
    "]\n",
    "image_format = \".bmp\"\n",
    "\n",
    "data_paths = list(map(os.path.abspath, data_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are naming convention in ALL_2 dataset\n",
    "<img src=\"ipynb_resources/ALL_CELLS_PHOTO_NAME_CONVENTIONS.jpg\">\n",
    "<img src=\"ipynb_resources/NORMAL_CELLS_PHOTO_NAME_CONVENTIONS.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataset parameters\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = (256, 256)\n",
    "SEED = 322\n",
    "PREFETCH_BUFFER_SIZE = 100\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "CACHE_DIR = \"caches/ds_cache\"\n",
    "ds_params = dict(\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    class_names=[\"all\", \"hem\"],\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sweet way to load data\n",
    "training_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_paths[0], subset=\"training\", shuffle=True,\n",
    "    validation_split=0.2, **ds_params \n",
    ")\n",
    "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_paths[0], subset=\"validation\",\n",
    "    validation_split=0.2, **ds_params\n",
    ")\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_paths[0], **ds_params\n",
    ")\n",
    "# calculate class scewness\n",
    "path_iter = os.walk(data_paths[0])\n",
    "number_of_all_examples, number_of_hem_examples = 0, 0\n",
    "for item in path_iter:\n",
    "    if 'all' in item[0]:\n",
    "        number_of_all_examples = len(item[2])\n",
    "    elif 'hem' in item[0]:\n",
    "        number_of_hem_examples = len(item[2])\n",
    "class_weights = {0: 1, 1: number_of_all_examples/number_of_hem_examples}\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "    result = normalization_layer(image)\n",
    "    #result = tf.image.resize(result, IMAGE_SIZE)\n",
    "    #result = tf.image.adjust_brightness(result, 0.1)\n",
    "    #result = tf.image.adjust_contrast(result, 1.3)\n",
    "    #result = tf.image.adjust_gamma(result, 1.5, 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_parallel_calls seems not working?\n",
    "training_ds = training_ds.map(lambda x,y: (preprocess_image(x), y), \n",
    "                              num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "                             )\n",
    "validation_ds = validation_ds.map(lambda x,y: (preprocess_image(x), y),\n",
    "                                 num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "                                 )\n",
    "test_ds = test_ds.map(lambda x,y: (preprocess_image(x), y),\n",
    "                                 num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "                                 )\n",
    "\n",
    "training_ds, validation_ds = training_ds.unbatch(), validation_ds.unbatch()\n",
    "\n",
    "training_ds = training_ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "validation_ds = validation_ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "training_ds = training_ds.cache(CACHE_DIR).prefetch(PREFETCH_BUFFER_SIZE)\n",
    "validation_ds = validation_ds.cache(CACHE_DIR).prefetch(PREFETCH_BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch in training_ds.take(1):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model0(input_shape):\n",
    "    seed = 32\n",
    "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=seed)\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), strides=(1,1), padding=\"valid\", \n",
    "                     input_shape=input_shape, kernel_initializer=weight_initializer))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), strides=(1,1), padding=\"valid\", \n",
    "              kernel_initializer=weight_initializer))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), strides=(1,1), padding=\"valid\", \n",
    "              kernel_initializer=weight_initializer))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(64))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv2D(96, (11, 11), strides=(4, 4), padding=\"valid\", input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(256, (5, 5), padding=\"same\"))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(384, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(384, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    #print(model.summary())\n",
    "    model.add(Dense(2048))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(2048))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_model0(IMAGE_SIZE + (3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rms = tf.keras.optimizers.RMSprop(\n",
    "      lr=0.001, rho=0.9, momentum=0.7, centered=True)\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.005, amsgrad=True)\n",
    "adadelta = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95)\n",
    "adagrad = tf.keras.optimizers.Adagrad(learning_rate=0.001,initial_accumulator_value=0.1)\n",
    "adamax = tf.keras.optimizers.Adamax(learning_rate=0.001)\n",
    "nadam = tf.keras.optimizers.Nadam(learning_rate=0.05)\n",
    "ftrl = tf.keras.optimizers.Ftrl(learning_rate=0.001,\n",
    "                                learning_rate_power=-0.5,\n",
    "                                initial_accumulator_value=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    tf.keras.metrics.Accuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), \n",
    "    tf.keras.metrics.AUC()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=adam, \n",
    "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "  metrics=METRICS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for layer in model.layers[:85]:\n",
    "   #layer.trainable = False\n",
    "#for layer in model.layers[85:]:\n",
    "  # layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set up TB logs\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, \n",
    "                                                     profile_batch = '500,520')\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"weights/\",\n",
    "    save_weights_only=True,\n",
    "    monitor='val_precision',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "#class_weights = {0: 1, 1: 1}\n",
    "# Fit model\n",
    "EPOCHS = 10\n",
    "hist = model.fit(training_ds,\n",
    "      epochs=EPOCHS,\n",
    "      validation_data=validation_ds,\n",
    "      callbacks=[tensorboard_callback, model_checkpoint_callback],\n",
    "      class_weight=class_weights\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,2])\n",
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.plot(hist.history[\"val_loss\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.ylabel(\"Accuracy (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,1])\n",
    "plt.plot(hist.history[\"accuracy\"])\n",
    "plt.plot(hist.history[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_false, all_true, hem_false, hem_true = 0, 0, 0, 0\n",
    "prediction_scores = model.predict(test_ds.map(lambda x, _ : x))\n",
    "print(\"Predicted\")\n",
    "count = 0\n",
    "for item in test_ds:\n",
    "    values = item[1]\n",
    "    for ind in range(len(values)):\n",
    "        if values[ind] == 0:\n",
    "            if prediction_scores[count * 32 + ind] <= 0.5:\n",
    "                all_true += 1\n",
    "            else:\n",
    "                all_false += 1\n",
    "        else:\n",
    "            if prediction_scores[count * 32 + ind] > 0.5:\n",
    "                hem_true += 1\n",
    "            else:\n",
    "                hem_false += 1\n",
    "#plt.figure()\n",
    "#plt.imshow(batch[0][ind])\n",
    "#plt.xlabel(get_class_string_from_index(true_index) + \"\\n\" + \n",
    "          #get_class_string_from_index(predicted_index))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(prediction_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"True ALL: \", all_true)\n",
    "print(\"False ALL: \", all_false)\n",
    "print(\"True/(Total_all): {:.4f}\".format(all_true / (all_false+all_true)))\n",
    "print(\"#\"*60)\n",
    "print(\"True hem: \", hem_true)\n",
    "print(\"False hem: \", hem_false)\n",
    "print(\"True/(Total_hem): {:.4f}\".format(hem_true / (hem_false+hem_true)))\n",
    "print(\"#\"*60)\n",
    "print(\"Total true: \", all_true + hem_true)\n",
    "print(\"Total false: \", all_false + hem_false)\n",
    "print(\"True/Total: {:.4f}\".format((all_true + hem_true) / (hem_false+hem_true+all_false+all_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D://DATASETS/ALL_2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\n",
    "    dataset_path + \"/C-NMC_test_prelim_phase_data/C-NMC_test_prelim_phase_data_labels.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Patient_ID new_names  labels\n",
      "0   UID_57_29_1_all.bmp     1.bmp       1\n",
      "1   UID_57_22_2_all.bmp     2.bmp       1\n",
      "2   UID_57_31_3_all.bmp     3.bmp       1\n",
      "3  UID_H49_35_1_hem.bmp     4.bmp       0\n",
      "4   UID_58_6_13_all.bmp     5.bmp       1\n"
     ]
    }
   ],
   "source": [
    "# 1 - all\n",
    "# 0 - hem\n",
    "print(labels.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
